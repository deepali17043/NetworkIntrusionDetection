# -*- coding: utf-8 -*-
"""project_evaluating_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hIuGckB9nQxSpPyR2KRc76W767aS-7qH

# Imports/Misc

Note: If running for small dataset on Google Colab, uncomment the following three lines/cells
"""

# !pip install pyspark

# %cd /content/drive/MyDrive/Summer24/BigData/Project/
# replace path-to-project with your working directory that also has the small dataset.

# !ls
# confirm that the input file is present

import sys
import numpy as np

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.stat import Correlation
from pyspark.ml import Pipeline
from pyspark.mllib.evaluation import MulticlassMetrics

# Initialize Spark Session
spark = SparkSession.builder.appName('NIDS_eval').getOrCreate()

"""Uncomment the following two lines if running for small dataset on Google Colab"""

# sys.argv[1] = 'small_NF_UQ_NIDS_v2.csv'
# sys.argv[2] = 'project_output_dir'

input_file = sys.argv[1]
output_dir = sys.argv[2]

output_log = []

"""# Read Data and EDA"""

# Load the data
data = spark.read.csv(input_file, header=True, inferSchema=True)

data_schema = data.schema
log = f'Data Read successful, schema infered:\n{data_schema}'

data.printSchema()
output_log.append(log)

data = data.drop('Dataset')

output_label = 'Label'
output_attack = 'Attack_index'
output_columns = [output_label, output_attack]

categorical_columns = [field for (field, dataType) in data.dtypes if dataType == "string"]

data_desc = data.describe()
log = f'Data Description:\n{data_desc}'
data_desc.show()
output_log.append(log)

# Index Catagorical columns to get correlation matrix
indexers = [StringIndexer(inputCol=column, outputCol=column+"_index") for column in categorical_columns]
encoders = [OneHotEncoder(inputCol=column+"_index", outputCol=column+"_encoded") for column in categorical_columns if column != 'Attack']
pipeline = Pipeline(stages=indexers+encoders)
data = pipeline.fit(data).transform(data)

data = data.drop(*categorical_columns)
indexed_cols = [column+"_index" for column in categorical_columns if column != 'Attack']
data = data.drop(*indexed_cols)

updated_schema = data.schema
log = f'Data Schema after indexing and encoding:\n{updated_schema}'

data.printSchema()
output_log.append(log)

log = 'Preprocessing complete'
print(log)
output_log.append(log)

"""#### Assemble features and create train-test split"""

feature_columns = [field for (field, dataType) in data.dtypes if (dataType in ['double', 'int']) & (field not in output_columns)]

# Assemble features
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data = assembler.transform(data)

# Select label and features columns
data_multi = data.select("features", "Attack_index")
data_binary = data.select("features", "Label")

# Split the data
(Train_multi, Test_multi) = data_multi.randomSplit([0.7, 0.3], seed=42)
(Train_binary, Test_binary) = data_binary.randomSplit([0.7, 0.3], seed=42)

log = 'Data Splits created'
print(log)
output_log.append(log)

"""# Random Forest Classifier

### Binary Classification - Attack (1) or not (0)
"""

log = 'Binary Classification - Attack (1) or not (0) - starting\nLoading saved model...'
print(log)
output_log.append(log)

rf_model_binary = None

try:
    rf_model_binary = RandomForestClassificationModel.load(f'{output_dir}/best_model_binary')
    log = f'Best Model for binary classification loaded from {output_dir}/best_model_binary'
    print(log)
    output_log.append(log)
except:
    log = f'No saved model found at {output_dir}/best_model_binary'
    print(log)
    output_log.append(log)

importances = rf_model_binary.featureImportances
feat_imp = [f'{feature_columns[i]}: {importances[i]}' for i in range(len(feature_columns))]
feat_imp = sorted(feat_imp, key=lambda x: float(x.split(':')[1]), reverse=True)
feat_imp = '\n'.join(feat_imp)
log = f'Feature importances:\n{feat_imp}'
print(log)
output_log.append(log)

if rf_model_binary is not None:
    predictions = rf_model_binary.transform(Test_binary)

    preds_and_labels = predictions.select(['prediction','Label']).rdd.map(lambda row: (float(row['prediction']), float(row['Label'])))
    metrics = MulticlassMetrics(preds_and_labels)

    conf_matrix = metrics.confusionMatrix().toArray()
    log = f'Confusion Matrix:\n{conf_matrix}'
    print(log)
    output_log.append(log)

    FPR = metrics.falsePositiveRate(0.0)
    log = f'False Positive Rate: {FPR}'
    print(log)
    output_log.append(log)

    TPR = metrics.truePositiveRate(0.0)
    log = f'True Positive Rate: {TPR}'
    print(log)
    output_log.append(log)

    precision = metrics.precision(1.0)
    log = f'Precision: {precision}'
    print(log)
    output_log.append(log)

    recall = metrics.recall(1.0)
    log = f'Recall: {recall}'
    print(log)
    output_log.append(log)

    f1 = metrics.fMeasure(1.0)
    log = f'F1 Score: {f1}'
    print(log)
    output_log.append(log)

    accuracy = metrics.accuracy
    log = f'Accuracy: {accuracy}'
    print(log)
    output_log.append(log)

    log = 'Binary Classification - Attack (1) or not (0) - complete'
    print(log)
    output_log.append(log)

"""### Multi-class classification - Attack type"""

log = 'Multi-class classification - Attack type - starting\nLoading saved model...'
print(log)
output_log.append(log)

rf_model_multi = None

try:
    rf_model_multi = RandomForestClassificationModel.load(f'{output_dir}/best_model_multiclass')
    log = f'Best Model for multi-class classification loaded from {output_dir}/best_model_multiclass'
    print(log)
    output_log
except:
    log = f'No saved model found at {output_dir}/best_model_multiclass'
    print(log)
    output_log.append(log)

importances = rf_model_multi.featureImportances
feat_imp = [f'{feature_columns[i]}: {importances[i]}' for i in range(len(feature_columns))]
feat_imp = sorted(feat_imp, key=lambda x: float(x.split(':')[1]), reverse=True)
feat_imp = '\n'.join(feat_imp)
log = f'Feature importances:\n{feat_imp}'
print(log)
output_log.append(log)

if rf_model_multi is not None:
    predictions = rf_model_multi.transform(Test_multi)

    preds_and_labels = predictions.select(['prediction','Attack_index']).rdd
    metrics = MulticlassMetrics(preds_and_labels)

    # conf_matrix = metrics.confusionMatrix().toArray()
    # log = f'Confusion Matrix:\n{conf_matrix}'
    # print(log)
    # output_log.append(log)

    FPR = metrics.falsePositiveRate(1.0)
    log = f'False Positive Rate: {FPR}'
    print(log)
    output_log.append(log)

    TPR = metrics.truePositiveRate(1.0)
    log = f'True Positive Rate: {TPR}'
    print(log)
    output_log.append(log)

    precision = metrics.precision(1.0)
    log = f'Precision: {precision}'
    print(log)
    output_log.append(log)

    recall = metrics.recall(1.0)
    log = f'Recall: {recall}'
    print(log)
    output_log.append(log)

    f1 = metrics.fMeasure(1.0)
    log = f'F1 Score: {f1}'
    print(log)
    output_log.append(log)

    accuracy = metrics.accuracy
    log = f'Accuracy: {accuracy}'
    print(log)
    output_log.append(log)

    log = 'Multi-class classification - Attack type - complete'
    print(log)
    output_log.append(log)

"""# Write output to output file and conclude"""

output_rdd = spark.sparkContext.parallelize(output_log)
output_rdd.saveAsTextFile(f'{output_dir}/evaluation_logs')

spark.stop()