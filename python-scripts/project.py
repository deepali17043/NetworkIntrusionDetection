# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TgbTWJw_xxuhPvBJnezMhdO1ISjPvCP9

# Imports/Misc
"""

# !pip install pyspark

# %cd /content/drive/MyDrive/Summer24/BigData/Project

# !ls

import sys
import numpy as np

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.stat import Correlation
from pyspark.ml import Pipeline

# Initialize Spark Session
spark = SparkSession.builder.appName('NetworkIntrusionDetection').getOrCreate()

# sys.argv[1] = 'small_NF_UQ_NIDS_v2.csv'
# sys.argv[2] = 'project_output_dir'

input_file = sys.argv[1]
output_dir = sys.argv[2]

output_log = []

"""# Read Data and EDA"""

# Load the data
data = spark.read.csv(input_file, header=True, inferSchema=True)

data_schema = data.schema
log = f'Data Read successful, schema infered:\n{data_schema}'
print(log)
output_log.append(log)

data = data.drop('Dataset')

output_label = 'Label'
output_attack = 'Attack_index'
output_columns = [output_label, output_attack]

categorical_columns = [field for (field, dataType) in data.dtypes if dataType == "string"]

data_desc = data.describe()
log = f'Data Description:\n{data_desc}'
print('Data Description:\n')
data_desc.show()
output_log.append(log)

# Index Catagorical columns to get correlation matrix
indexers = [StringIndexer(inputCol=column, outputCol=column+"_index") for column in categorical_columns]
encoders = [OneHotEncoder(inputCol=column+"_index", outputCol=column+"_encoded") for column in categorical_columns if column != 'Attack']
pipeline = Pipeline(stages=indexers+encoders)
data = pipeline.fit(data).transform(data)

data = data.drop(*categorical_columns)
indexed_cols = [column+"_index" for column in categorical_columns if column != 'Attack']
data = data.drop(*indexed_cols)

updated_schema = data.schema
log = f'Data Schema after indexing and encoding:\n{updated_schema}'
print(log)
output_log.append(log)

feature_columns = [field for (field, dataType) in data.dtypes if (dataType in ['double', 'int']) & (field not in output_columns)]

label_corr = [(col, data.stat.corr(col, output_label)) for col in feature_columns]
log = f'Correlation with Label:\n{label_corr}'
print(log)
output_log.append(log)

attack_corr = [(col, data.stat.corr(col, output_attack)) for col in feature_columns]
log = f'Correlation with Attack_index:\n{attack_corr}'
print(log)
output_log.append(log)

# Assemble features
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data = assembler.transform(data)

# Select label and features columns
data_multi = data.select("features", "Attack_index")
data_binary = data.select("features", "Label")

# Split the data
(Train_multi, Test_multi) = data_multi.randomSplit([0.7, 0.3], seed=42)
(Train_binary, Test_binary) = data_binary.randomSplit([0.7, 0.3], seed=42)

log = 'EDA Completed & Data Splits created'
print(log)
output_log.append(log)

"""# Random Forest Classifier

### Binary Classification - Attack (1) or not (0)
"""

log = 'Binary Classification - Attack (1) or not (0) - starting'
print(log)
output_log.append(log)

# Train a RandomForest model
rf = RandomForestClassifier(labelCol="Label", featuresCol="features", numTrees=10)

# Train model
model = rf.fit(Train_binary)

# Make predictions
predictions = model.transform(Test_binary)

# Evaluate the model
evaluator = BinaryClassificationEvaluator(labelCol="Label", metricName="areaUnderROC")
areaUnderROC = evaluator.evaluate(predictions)
log = f"Test areaUnderROC - with standard RF model: {areaUnderROC}"
print(log)
output_log.append(log)

# Hyperparameter tuning
paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20, 30]).build()
crossval = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)
cvModel_bin = crossval.fit(Train_binary)

# Make predictions with the best model
predictions = cvModel_bin.transform(Test_binary)
areaUnderROC = evaluator.evaluate(predictions)
log = f"Test areaUnderROC - with tuned RF model (using 3-fold CrossValidation): {areaUnderROC}"
print(log)
output_log.append(log)

cvModel_bin.bestModel.save(output_dir+'/best_model_binary')
log = f'Best Model for binary classification saved to {output_dir}/best_model_bin'
print(log)
print(f'Best Model: {cvModel_bin.bestModel}')
output_log.append(log)

"""### Multi-class classification - Attack type"""

log = 'Multi-class classification - Attack type - starting'
print(log)
output_log.append(log)

# Train a RandomForest model
rf = RandomForestClassifier(labelCol="Attack_index", featuresCol="features", numTrees=10)

# Train model
model = rf.fit(Train_multi)

# Make predictions
predictions = model.transform(Test_multi)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="Attack_index", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
log = f"Test Accuracy - with standard RF model: {accuracy}"
print(log)
output_log.append(log)

# Hyperparameter tuning
paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20, 30]).build()
crossval = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)
cvModel_multi = crossval.fit(Train_multi)

# Make predictions with the best model
predictions = cvModel_multi.transform(Test_multi)
accuracy = evaluator.evaluate(predictions)
log = f"Test Accuracy - with tuned RF model (using 3-fold CrossValidation): {accuracy}"
print(log)
output_log.append(log)

cvModel_multi.bestModel.save(output_dir+'/best_model_multiclass')
log = f'Best Model for multi-class classification saved to {output_dir}/best_model_multi'
print(log)
print(f'Best Model: {cvModel_multi.bestModel}')
output_log.append(log)

"""# Write output to output file and conclude"""

output_rdd = spark.sparkContext.parallelize(output_log)
output_rdd.saveAsTextFile(f'{output_dir}/output_logs')

spark.stop()